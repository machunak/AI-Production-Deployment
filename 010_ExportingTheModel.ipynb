{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wefwYngnby0K"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"헤더\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8VGepdAby0M"
   },
   "source": [
    "# 1.0 모델 내보내기\n",
    "이 노트북에서는 PyTorch를 사용해 트레이닝된 BERT 체크포인트를 NVIDIA Triton Inference Server로 내보내는 옵션을 살펴보겠습니다.\n",
    "\n",
    "**[1.1 개요: 최적화 및 성능](#1.1-개요:-최적화-및-성능)<br>**\n",
    "**[1.2 BERT 체크포인트 내보내기](#1.2-BERT-체크포인트-내보내기)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.1 Triton 모델 리포지토리](#1.2.1-Triton-모델-리포지토리)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.2 TorchScript 내보내기](#1.2.2-TorchScript-내보내기)<br>\n",
    "**[1.3 내보내기 테스트](#1.3-내보내기-테스트)<br>**\n",
    "**[1.4 TorchScript 외](#1.4-TorchScript-외)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.4.1 연습: TensorRT 최적화 활성화](#1.4.1-연습:-TensorRT-최적화-활성화)<br>\n",
    "**[1.5 성능 비교](#1.5-성능-비교)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWBESdu9by0N"
   },
   "source": [
    "# 1.1 개요: 최적화 및 성능\n",
    "트레이닝된 모델 최적화는 대역폭 및 지연 시간으로 측정되는 추론 성능에 매우 큰 영향을 미칩니다. 프로젝트 요구 사항이 지식 증류 또는 프루닝과 같은 고급 기법에 엔지니어링 노력을 투자하는 것을 정당화하지는 않지만 모델 최적화 도구를 사용하여 상당한 모델 성능 개선을 달성할 수 있습니다. 아래 다이어그램은 최적화되지 않은 TensorFlow를 사용하여 배포된 모델, TensorRT로 사후 처리된 동일한 모델, 그리고 TensorRT로 완전히 최적화된 모델 간의 추론 성능 차이를 보여줍니다. \n",
    "\n",
    "<img src=\"images/TFvTRT.jpg\" alt=\"헤더\" style=\"width: 600px;\"/>\n",
    "\n",
    "최신 추론 서버는 일반적으로 둘 이상의 모델 형식을 실질적으로 지원하여 더 광범위한 프로젝트, 도구 및 환경 설정을 충족합니다. 이 수업에서는 PyTorch를 사용해 트레이닝된 BERT 체크포인트를 사용하고 Triton Inference Server에 배포하므로 PyTorch 기반 모델을 배포하는 옵션을 중점적으로 살펴보겠습니다. 이러한 옵션에는 다음이 포함됩니다.\n",
    "   - PyTorch JIT/TorchScript\n",
    "   - ONNX 런타임\n",
    "   - ONNX-TensorRT\n",
    "   - TensorRT\n",
    "    \n",
    "Triton Server가 다음을 포함해 훨씬 더 폭넓은 배포 메커니즘 세트를 지원한다는 점에 주목해야 합니다.\n",
    "   - TensorFlow GraphDef\n",
    "   - TensorFlow 저장 모델\n",
    "   - Caffe 2 내보내기\n",
    "   - 맞춤형 모델(어떤 맞춤형 실행 파일도 될 수 있음)\n",
    "\n",
    "이 섹션에서는 위에 열거된 배포 엔진 중 일부를 사용해 모델을 배포하는 방법과 각 옵션이 성능에 미치는 영향을 살펴보겠습니다. 또한 몇몇 주요 설정 즉, 배치 크기와 숫자 정밀도(FP32 및 FP16)를 실험해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMopPhDoby0O"
   },
   "source": [
    "# 1.2 BERT 체크포인트 내보내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEOOzdZ-by0O"
   },
   "source": [
    "배포할 BERT 모델 체크포인트인 <code>bert_qa.pt</code>는 `data` 디렉토리에 위치해 있어야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pyt2-Cxyby0P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bert_qa.pt\n"
     ]
    }
   ],
   "source": [
    "!ls data/*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D480O_uby0P"
   },
   "source": [
    "이 파일은 BERT-Large 네트워크의 표준 체크포인트로, [SQuAD(Stanford Question Answering Dataset)](https://arxiv.org/abs/1606.05250)에서 파인튜닝(Fine-tuning)되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlQMYURwby0P"
   },
   "source": [
    "#### 헬퍼 스크립트\n",
    "우리는 다양한 배포 구성을 살펴보면서 일부 단계를 여러 번 반복할 것입니다.  따라서 구성 설정 및 결과에 주의를 집중할 수 있도록 몇 가지 헬퍼 스크립트를 사용하여 프로세스를 부분적으로 자동화할 것입니다.  관심이 있다면 코드 세부 정보를 직접 살펴볼 수 있습니다.\n",
    "\n",
    "- [utilities/wait_for_triton_server.sh](utilities/wait_for_triton_server.sh): API를 통해 Triton Server의 \"라이브\" 및 \"준비\" 상태를 확인합니다.\n",
    "- [deployer/deployer.py](deployer/deployer.py): 체크포인트를 배포 가능한 모델로 변환해서 내보냅니다.\n",
    "- [utilities/run_perf_analyzer_local.sh](utilities/run_perf_analyzer_local.sh): [perf_analyzer](https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md) 애플리케이션을 사용해 성능을 측정합니다.\n",
    "- [utilities/run_warmup.sh](utilities/run_warmup.sh): `perf_analyzer`를 사용하여 몇 가지 추론을 실행하여 모델을 웜업합니다. 모델을 웜업하면 더 안정적인 측정값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW5RvKNQby0Q"
   },
   "source": [
    "Triton Server가 컨테이너에 배포되었으며 포트 \"8000\"의 호스트 \"triton\"에서 사용할 수 있습니다. 다음 셀을 실행하여 API에서 \"200 OK\" HTTP 응답을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yqOkoVHfby0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfYLdoA3by0Q"
   },
   "source": [
    "## 1.2.1 Triton 모델 리포지토리\n",
    "Triton Server가 시작되면 일반적으로 이 서버는 모델이 호스팅되는 로컬 또는 원격 파일 시스템을 관찰하도록 구성됩니다. 관찰되는 디렉토리를 *모델 리포지토리*라고 합니다. Triton Server를 시작하는 일반적인 명령은 옵션을 사용하여 모델 리포지토리의 위치를 식별합니다.<br>\n",
    "```bash\n",
    "tritonserver --model-repository=\"/path/to/model/repository\"\n",
    "```\n",
    "\n",
    "모델 리포지토리는 다음 레이아웃을 사용해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRlOQ27xby0Q"
   },
   "source": [
    "```python\n",
    "<model-repository-path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRn1pGSQby0R"
   },
   "source": [
    "이 랩 컨테이너는 <code>./model_repository</code> 폴더를 모델 리포지토리로 사용하도록 구성되어 있으므로 이 폴더 내의 모든 변경 사항이 Triton Server의 동작에 영향을 미칩니다.<br/>\n",
    "\n",
    "새로운 모델을 Triton에 노출하려면 다음을 수행해야 합니다. <br/>\n",
    "   1. 모델 리포지토리에 새 모델 폴더를 만듭니다. 폴더의 이름은 사용자/애플리케이션에 노출할 서비스의 이름을 반영해야 합니다.<br/>\n",
    "   2. 모델 폴더 내에서 해당 모델의 기본 서빙 구성을 포함하는 <code>config.pbtxt</code> 파일을 만듭니다.<br/>\n",
    "   3. 또한 모델 폴더 내에서 모델의 복사본을 포함하는 폴더를 적어도 하나 이상 만듭니다. 폴더의 이름은 모델의 버전 이름을 반영합니다. 동일한 모델의 여러 버전을 만들고 호스팅할 수 있습니다.<br/>\n",
    "    \n",
    "다음으로, 모델을 Triton으로 내보내는 프로세스를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVu767o8by0R"
   },
   "source": [
    "## 1.2.2 TorchScript 내보내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6NNr0Beby0R"
   },
   "source": [
    "랩의 이 부분에서는 다음 작업을 수행합니다.\n",
    "   - PyTorch 체크포인트를 [TorchScript](https://pytorch.org/docs/stable/jit.html#torchscript)로 변환\n",
    "   - Triton 구성 파일 생성\n",
    "   - 생성된 자산을 모델 리포지토리에 배포\n",
    "아래 셀을 실행하십시오. PyTorch 체크포인트를 로드하고 TorchScript로 변환하는 데에는 1~2분 정도 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Tza1bophby0R"
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pGSrC0UVby0R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-torchscript in format pytorch_libtorch\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_recursive.py:260: UserWarning: 'bias' was found in ScriptModule constants,  but it is a non-constant parameter. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.02516031265258789\n",
      "\n",
      "average L_inf error over output tensors:  0.013570129871368408\n",
      "variance of L_inf error over output tensors:  0.00011137690313015962\n",
      "stddev of L_inf error over output tensors:  0.010553525625598283\n",
      "\n",
      "time of error check of native model:  0.7686853408813477 seconds\n",
      "time of error check of ts model:  1.4161465167999268 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --ts-script \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint \"/dli/task/data/bert_qa.pt\" \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfWNLla6by0S"
   },
   "source": [
    "`deployer.py` 스크립트가 `bert_qa.pt` 체크포인트를 로드하고 이를 `ts-script` 형식으로 `bertQA-torchscript`라는 폴더에 배포한 다음 버전 `1`로 표시합니다. 더 고급에 해당하는 설정은 나중에 설명하겠습니다. 지금은 스크립트에 의해 생성된 파일을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r0ERZNKtby0S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Mar  4 05:55 .\n",
      "drwxr-xr-x 3 root root 4096 Mar  4 05:55 ..\n",
      "drwxr-xr-x 2 root root 4096 Mar  4 05:55 1\n",
      "-rw-r--r-- 1 root root  568 Mar  4 05:55 config.pbtxt\n",
      "total 1309300\n",
      "drwxr-xr-x 2 root root       4096 Mar  4 05:55 .\n",
      "drwxr-xr-x 3 root root       4096 Mar  4 05:55 ..\n",
      "-rw-r--r-- 1 root root 1340709950 Mar  4 05:55 model.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-torchscript/\n",
    "!ls -al ./candidatemodels/bertQA-torchscript/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L-orwUGby0S"
   },
   "source": [
    "예상한 대로, 스크립트가 모델을 TorchScript 형식으로 내보낸 다음 `model.pt`로 저장했습니다. 또한 `config.pbtxt` 파일을 생성했습니다. <br> \n",
    "한번 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WskWpsf4by0S",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"bertQA-torchscript\"\n",
      "platform: \"pytorch_libtorch\"\n",
      "max_batch_size: 8\n",
      "input [\n",
      "{\n",
      "    name: \"input__0\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__1\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__2\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "output [\n",
      "{\n",
      "    name: \"output__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}, \n",
      "{\n",
      "    name: \"output__1\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "optimization {\n",
      "  cuda {\n",
      "    graphs: 0\n",
      "  }\n",
      "}\n",
      "instance_group [\n",
      "    {\n",
      "        count: 1\n",
      "        kind: KIND_GPU\n",
      "        gpus: [ 0 ]\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat ./candidatemodels/bertQA-torchscript/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXvUnVQAby0S"
   },
   "source": [
    "구성 파일은 매우 단순하며 다음을 정의합니다.\n",
    "   - 모델의 이름\n",
    "   - 추론에 사용할 플랫폼의 유형, 이 경우에는 `pytorch_libtorch`\n",
    "   - 네트워크에서 사용되는 입력 및 출력 차원\n",
    "   - 사용된 최적화, 이 경우에는 GPU 및 기본 TorchScript 최적화 \n",
    "   - 인스턴스 그룹 구성, 이 경우에는 인스턴스 그룹 수가 1로 설정되며, 이는 모델 복사본 한 개만 GPU 메모리(GPU 0이 사용됨)에 저장됨을 의미합니다.\n",
    "    \n",
    "모델을 배포하려면 폴더를 Triton 모델 리포지토리로 이동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZT2s6e2-by0T"
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-torchscript model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq9sYHRJby0T"
   },
   "source": [
    "축하합니다!  첫 번째 모델을 Triton Inference Server에 성공적으로 배포했습니다!\n",
    "\n",
    "나중에 다시 돌아와서 자세한 구성에 대해 이야기하겠지만 지금은 모델이 수행되는 방식을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rODO9OQZby0T"
   },
   "source": [
    "#  1.3 내보내기 테스트\n",
    "아래 셀을 실행하여 추론 프로세스를 시작하고 추론 성능을 간단하게 측정합니다. 먼저, 일부 구성을 설정하겠습니다. `maxConcurrency`는 2로 설정되며, 이는 스트레스 테스트가 두 번 실행됨을 의미합니다. 첫 번째 실행에서는 단일 스레드만 사용하고 두 번째 실행에서는 2개의 스레드를 사용하여 서버를 쿼리합니다. 동시 모델 실행 또는 동적 배치 기능을 켜지 않으면 서버를 쿼리하는 두 개의 프로세스 실행 성능에 어떤 영향을 미칠거라고 생각하십니까?<br/>\n",
    "- 대역폭이 증가할까요, 감소할까요?<br/>\n",
    "- 지연 시간이 증가할까요, 감소할까요?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BRZRYZLjby0T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelVersion = \"1\"\n",
    "precision = \"fp32\"\n",
    "batchSize = \"8\"\n",
    "maxLatency = \"500\"\n",
    "maxClientThreads = \"10\"\n",
    "maxConcurrency = \"2\"\n",
    "dockerBridge = \"host\"\n",
    "resultsFolderName = \"1\"\n",
    "profilingData = \"utilities/profiling_data_int64\"\n",
    "measurement_request_count = 50\n",
    "percentile_stability = 85\n",
    "stability_percentage = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HL2qQ43mby0T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-torchscript\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 2 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 35.33 infer/sec. p85 latency: 141283 usec\n",
      "  Pass [2] throughput: 57.1356 infer/sec. p85 latency: 141775 usec\n",
      "  Pass [3] throughput: 55.9936 infer/sec. p85 latency: 142262 usec\n",
      "  Pass [4] throughput: 55.9924 infer/sec. p85 latency: 142523 usec\n",
      "  Client: \n",
      "    Request count: 162\n",
      "    Throughput: 56.3407 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 7.05 infer/sec\n",
      "    p50 latency: 141768 usec\n",
      "    p85 latency: 142262 usec\n",
      "    p90 latency: 142482 usec\n",
      "    p95 latency: 142672 usec\n",
      "    p99 latency: 142784 usec\n",
      "    Avg gRPC time: 141670 usec (marshal 5 usec + response wait 141664 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1296\n",
      "    Execution count: 162\n",
      "    Successful request count: 162\n",
      "    Avg request latency: 141432 usec (overhead 88 usec + queue 27 usec + compute input 46 usec + compute infer 141224 usec + compute output 46 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 57.1358 infer/sec. p85 latency: 282668 usec\n",
      "  Pass [2] throughput: 57.1347 infer/sec. p85 latency: 283099 usec\n",
      "  Pass [3] throughput: 55.9931 infer/sec. p85 latency: 283134 usec\n",
      "  Client: \n",
      "    Request count: 156\n",
      "    Throughput: 56.7199 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 7.14 infer/sec\n",
      "    p50 latency: 282306 usec\n",
      "    p85 latency: 283102 usec\n",
      "    p90 latency: 283132 usec\n",
      "    p95 latency: 283157 usec\n",
      "    p99 latency: 283241 usec\n",
      "    Avg gRPC time: 280447 usec (marshal 6 usec + response wait 280439 usec + unmarshal 2 usec)\n",
      "  Server: \n",
      "    Inference count: 1248\n",
      "    Execution count: 156\n",
      "    Successful request count: 156\n",
      "    Avg request latency: 280170 usec (overhead 88 usec + queue 139071 usec + compute input 49 usec + compute infer 140910 usec + compute output 51 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 56.3407 infer/sec, latency 142262 usec\n",
      "Concurrency: 2, throughput: 56.7199 infer/sec, latency 283102 usec\n",
      "CPU times: user 438 ms, sys: 158 ms, total: 596 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-torchscript\"\n",
    "maxConcurrency = \"2\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0N445CXby0U"
   },
   "source": [
    "모든 것이 정상일 경우 다음 예제 결과와 비슷한 출력이 표시되어 두 가지 구성 전반에서의 추론 성능을 보여주어야 합니다.<br/>\n",
    "<img src=\"images/InferenceJob1.png\" alt=\"추론 작업 1의 예제 출력\" style=\"width: 1200px;\"/>\n",
    "\n",
    "\"오류: 모델 메타데이터를 가져오지 못했습니다.\"라는 메시지가 표시될 경우 셀을 다시 실행해 보십시오.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBbix9hWby0U"
   },
   "source": [
    "# 1.4 TorchScript 외\n",
    "\n",
    "모델을 Triton에 배포하는 다른 경로인 <a href=\"https://onnx.ai\">ONNX(Open Neural Network Exchange)</a>를 살펴보겠습니다. ONNX는 뉴럴 네트워크 모델을 표현하고 교환하는 오픈 형식입니다. ONNX는 공통 모델을 구축하는 데 사용되는 공통 연산자 세트뿐 아니라 그러한 모델을 교환하기 위한 파일 형식을 정의합니다. ONNX의 이점은 비교적 광범위하게 채택되며 딥러닝 프레임워크 또는 배포 도구와 같은 <a href=\"https://onnx.ai/supported-tools.html\">다양한 딥러닝 도구</a> 간에 모델을 교환하는 데 사용될 수 있다는 점입니다. 여기에는 ONNX 모델을 사용할 수 있는 TensorRT도 포함됩니다. </br>\n",
    "\n",
    "앞서와 마찬가지로 모델을 내보내는 것으로 시작하지만 이번에는 ONNX 형식을 사용합니다. 앞서 사용한 내보내기 도구를 활용하되, 내보내기 형식을 <code>ts-script</code>에서 <code>onnx</code>로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EUYLzl-Bby0U"
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cucJdT7Hby0V",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.325636625289917 seconds\n",
      "time of error check of onnx model:  9.888893127441406 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4QDD1FHby0V"
   },
   "source": [
    "<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md\">TorchScript 직렬화 형식</a>과 마찬가지로, <a href=\"https://onnx.ai/get-started.html\">ONNX 형식</a>은 매우 쉽게 확인할 수 있습니다(형식 부분을 사람이 판독할 수 있음). 내보내기에서 생성한 자산을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RvD3e6Ryby0V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Mar  4 06:03 .\n",
      "drwxr-xr-x 3 root root 4096 Mar  4 06:03 ..\n",
      "drwxr-xr-x 2 root root 4096 Mar  4 06:03 1\n",
      "-rw-r--r-- 1 root root  561 Mar  4 06:03 config.pbtxt\n",
      "total 1305596\n",
      "drwxr-xr-x 2 root root       4096 Mar  4 06:03 .\n",
      "drwxr-xr-x 3 root root       4096 Mar  4 06:03 ..\n",
      "-rw-r--r-- 1 root root 1336922015 Mar  4 06:03 model.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx/\n",
    "!ls -al ./candidatemodels/bertQA-onnx/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp930o6Mby0V"
   },
   "source": [
    "다시 한번 말하지만 모델뿐 아니라 구성 파일도 있으며 이번에는 ONNX 형식으로 저장되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8qId_q4by0W"
   },
   "source": [
    "Triton에서 ONNX 기반 내보내기를 실행할 수 있는 몇 가지 옵션이 있습니다.\n",
    "- ONNX 런타임을 활용할 수 있음 </br>\n",
    "- 대신 사용할 TensorRT 엔진을 생성하기 위해 TensorRT에 ONNX 자산 구문 분석을 요청할 수 있음 </br>\n",
    "\n",
    "두 접근 방식을 모두 시도해 보고 이것이 추론 성능에 미치는 영향을 살펴보겠습니다. 현재 ONNX 모델을 배포하기 위해 모델을 모델 리포지토리로 이동합니다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-JZZtJnkby0W"
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttRbgHo_by0W"
   },
   "source": [
    "...그런 다음 10가지 동시 실행 수준에서 스트레스 테스트 코드를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E9l_VYz4by0W",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 67.9906 infer/sec. p85 latency: 117405 usec\n",
      "  Pass [2] throughput: 67.9894 infer/sec. p85 latency: 117387 usec\n",
      "  Pass [3] throughput: 67.989 infer/sec. p85 latency: 117356 usec\n",
      "  Client: \n",
      "    Request count: 153\n",
      "    Throughput: 67.9897 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.55 infer/sec\n",
      "    p50 latency: 117306 usec\n",
      "    p85 latency: 117387 usec\n",
      "    p90 latency: 117407 usec\n",
      "    p95 latency: 117440 usec\n",
      "    p99 latency: 117880 usec\n",
      "    Avg gRPC time: 117315 usec (marshal 4 usec + response wait 117310 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1224\n",
      "    Execution count: 153\n",
      "    Successful request count: 153\n",
      "    Avg request latency: 117086 usec (overhead 68 usec + queue 28 usec + compute input 31 usec + compute infer 116949 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 67.9917 infer/sec. p85 latency: 234151 usec\n",
      "  Pass [2] throughput: 69.3232 infer/sec. p85 latency: 234065 usec\n",
      "  Pass [3] throughput: 67.9904 infer/sec. p85 latency: 234084 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4351 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 233994 usec\n",
      "    p85 latency: 234101 usec\n",
      "    p90 latency: 234120 usec\n",
      "    p95 latency: 234166 usec\n",
      "    p99 latency: 234197 usec\n",
      "    Avg gRPC time: 232893 usec (marshal 5 usec + response wait 232887 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 232585 usec (overhead 66 usec + queue 115596 usec + compute input 29 usec + compute infer 116884 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 67.9898 infer/sec. p85 latency: 351191 usec\n",
      "  Pass [2] throughput: 67.9897 infer/sec. p85 latency: 351236 usec\n",
      "  Pass [3] throughput: 69.3231 infer/sec. p85 latency: 351061 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4342 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 350995 usec\n",
      "    p85 latency: 351170 usec\n",
      "    p90 latency: 351216 usec\n",
      "    p95 latency: 351236 usec\n",
      "    p99 latency: 351318 usec\n",
      "    Avg gRPC time: 349247 usec (marshal 6 usec + response wait 349240 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 348924 usec (overhead 66 usec + queue 231930 usec + compute input 29 usec + compute infer 116887 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 67.9907 infer/sec. p85 latency: 468112 usec\n",
      "  Pass [2] throughput: 67.9901 infer/sec. p85 latency: 468020 usec\n",
      "  Pass [3] throughput: 69.3227 infer/sec. p85 latency: 467940 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4345 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 467910 usec\n",
      "    p85 latency: 468066 usec\n",
      "    p90 latency: 468085 usec\n",
      "    p95 latency: 468112 usec\n",
      "    p99 latency: 468166 usec\n",
      "    Avg gRPC time: 465494 usec (marshal 6 usec + response wait 465487 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 465189 usec (overhead 65 usec + queue 348220 usec + compute input 28 usec + compute infer 116865 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 67.9918 infer/sec. p85 latency: 584811 usec\n",
      "  Pass [2] throughput: 67.9902 infer/sec. p85 latency: 584992 usec\n",
      "  Pass [3] throughput: 67.99 infer/sec. p85 latency: 585066 usec\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 67.9897 infer/sec, latency 117387 usec\n",
      "Concurrency: 2, throughput: 68.4351 infer/sec, latency 234101 usec\n",
      "Concurrency: 3, throughput: 68.4342 infer/sec, latency 351170 usec\n",
      "Concurrency: 4, throughput: 68.4345 infer/sec, latency 468066 usec\n",
      "CPU times: user 677 ms, sys: 291 ms, total: 969 ms\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFrz_pRUby0X"
   },
   "source": [
    "결과를 살펴보십시오. 10개의 모든 동시 실행 수준에서 벤치마크를 실행했습니까, 또는 벤치마크가 조기에 시간 초과되었습니까? 우리가 구성한 500ms 시간 제한과 관련해서 요청 지연 시간은 어떻게 되었습니까?</br>\n",
    "\n",
    "이제 TensorRT 실행에 적합하게 구성할 수 있도록 ONNX 모델을 다시 살펴보겠습니다.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cocTFaf0by0X"
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "g5rYngH8by0Y",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-fp16 in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.32586193084716797 seconds\n",
      "time of error check of onnx model:  9.698695182800293 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aMf0Melby0Y"
   },
   "source": [
    "다시 한번 말하지만 위의 명령으로 구성 파일뿐 아니라 ONNX 내보내기까지 생성되어야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sOnwuc5iby0Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Mar  4 06:06 .\n",
      "drwxr-xr-x 3 root root 4096 Mar  4 06:05 ..\n",
      "drwxr-xr-x 2 root root 4096 Mar  4 06:06 1\n",
      "-rw-r--r-- 1 root root  570 Mar  4 06:06 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx-trt-fp16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-44VVh5kby0Y"
   },
   "source": [
    "## 1.4.1 연습: TensorRT 최적화 활성화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPjBOJCbby0Z"
   },
   "source": [
    "TensorRT를 활성화하려면 \"config.pbtxt\" 구성 파일에 추가 섹션을 추가해야 합니다. 특히, <code>최적화</code> 섹션에 추가 세그먼트를 추가해야 합니다.\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 연습 단계:\n",
    "1. TensorRT를 활성화하도록 [config.pbtxt](candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt)를 수정합니다. 필요에 따라 언제든지 [솔루션](solutions/ex-1-4-1_config.pbtxt)을 살펴보십시오.\n",
    "2. 변경 사항을 저장한 후(주 메뉴: 파일 -> 파일 저장), 아래 셀을 사용하여 폴더를 모델 리포지토리로 이동합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick fix!\n",
    "!cp solutions/ex-1-4-1_config.pbtxt candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jlRDmzYlby0Z"
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-fp16 model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxk3yOA1by0Z"
   },
   "source": [
    "3. 다음 셀에서 프로파일링 도구를 실행하여 성능에 미치는 영향을 확인합니다. 서버가 모델을 TensorRT로 마이그레이션할 때까지 기다려야 하므로 이 작업을 시작하려면 시간이 다소 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tC0QMjd0by0Z",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 67.9921 infer/sec. p85 latency: 117531 usec\n",
      "  Pass [2] throughput: 67.9883 infer/sec. p85 latency: 117447 usec\n",
      "  Pass [3] throughput: 67.989 infer/sec. p85 latency: 117449 usec\n",
      "  Client: \n",
      "    Request count: 153\n",
      "    Throughput: 67.9898 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.55 infer/sec\n",
      "    p50 latency: 117380 usec\n",
      "    p85 latency: 117469 usec\n",
      "    p90 latency: 117488 usec\n",
      "    p95 latency: 117533 usec\n",
      "    p99 latency: 117599 usec\n",
      "    Avg gRPC time: 117387 usec (marshal 5 usec + response wait 117381 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1224\n",
      "    Execution count: 153\n",
      "    Successful request count: 153\n",
      "    Avg request latency: 117137 usec (overhead 67 usec + queue 29 usec + compute input 31 usec + compute infer 116999 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 67.9893 infer/sec. p85 latency: 234340 usec\n",
      "  Pass [2] throughput: 67.9895 infer/sec. p85 latency: 234248 usec\n",
      "  Pass [3] throughput: 69.3212 infer/sec. p85 latency: 234244 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4333 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 234206 usec\n",
      "    p85 latency: 234298 usec\n",
      "    p90 latency: 234319 usec\n",
      "    p95 latency: 234340 usec\n",
      "    p99 latency: 234399 usec\n",
      "    Avg gRPC time: 233181 usec (marshal 6 usec + response wait 233174 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 232899 usec (overhead 65 usec + queue 115802 usec + compute input 29 usec + compute infer 116992 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 67.9903 infer/sec. p85 latency: 351348 usec\n",
      "  Pass [2] throughput: 67.988 infer/sec. p85 latency: 351263 usec\n",
      "  Pass [3] throughput: 67.9899 infer/sec. p85 latency: 351310 usec\n",
      "  Client: \n",
      "    Request count: 153\n",
      "    Throughput: 67.9894 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.55 infer/sec\n",
      "    p50 latency: 351215 usec\n",
      "    p85 latency: 351310 usec\n",
      "    p90 latency: 351333 usec\n",
      "    p95 latency: 351369 usec\n",
      "    p99 latency: 351448 usec\n",
      "    Avg gRPC time: 349617 usec (marshal 7 usec + response wait 349609 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1224\n",
      "    Execution count: 153\n",
      "    Successful request count: 153\n",
      "    Avg request latency: 349299 usec (overhead 65 usec + queue 232235 usec + compute input 27 usec + compute infer 116961 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 69.3247 infer/sec. p85 latency: 468438 usec\n",
      "  Pass [2] throughput: 67.9895 infer/sec. p85 latency: 468525 usec\n",
      "  Pass [3] throughput: 67.9904 infer/sec. p85 latency: 468511 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4349 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 468412 usec\n",
      "    p85 latency: 468508 usec\n",
      "    p90 latency: 468522 usec\n",
      "    p95 latency: 468551 usec\n",
      "    p99 latency: 468580 usec\n",
      "    Avg gRPC time: 465457 usec (marshal 6 usec + response wait 465450 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 465158 usec (overhead 66 usec + queue 348067 usec + compute input 26 usec + compute infer 116989 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 67.9918 infer/sec. p85 latency: 585630 usec\n",
      "  Pass [2] throughput: 69.3237 infer/sec. p85 latency: 585474 usec\n",
      "  Pass [3] throughput: 67.9898 infer/sec. p85 latency: 585518 usec\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 67.9898 infer/sec, latency 117469 usec\n",
      "Concurrency: 2, throughput: 68.4333 infer/sec, latency 234298 usec\n",
      "Concurrency: 3, throughput: 67.9894 infer/sec, latency 351310 usec\n",
      "Concurrency: 4, throughput: 68.4349 infer/sec, latency 468508 usec\n",
      "CPU times: user 696 ms, sys: 290 ms, total: 986 ms\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \" + modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0dknPjZby0Z"
   },
   "source": [
    "# 1.5 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlJsbyGJby0Z"
   },
   "source": [
    "마지막으로, ONNX 런타임을 기준으로 성능을 비교해 보겠습니다. \n",
    "* 특히 더 규모가 큰 동시 실행 전반에서 지연 시간이 어떻게 변경되었습니까? \n",
    "* 대역폭은 어떻게 변경되었습니까? 관찰된 대역폭 변경 수준을 설명해 주실 수 있습니까? \n",
    "* 10개 미만의 동시 실행에서 ONNX 모델이 시간 초과된 이유는 무엇입니까? 이전 동시 실행에서 순수 ONNX 런타임의 지연 시간과 비교할 때 동시 실행 10에서의 TensorRT 지연 시간은 어떻습니까?\n",
    "\n",
    "강사와 논의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgpNoit4by0a"
   },
   "source": [
    "<h3 style=\"color:green;\">축하합니다!</h3><br>\n",
    "TorchScript를 사용해 성공적으로 NLP 모델을 Triton Server에 배포했으며 감소된 정밀도와 TensorRT 최적화를 모두 적용했습니다.\n",
    "다음 노트북에서는 모델 자체를 최적화하고 효율적인 방식으로 배포하는 방법을 배우게 됩니다. \n",
    "\n",
    "다음 노트북을 진행하십시오.<br>\n",
    "[2.0 모델 호스팅](020_HostingTheModel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JetgKzviby0a"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"헤더\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "010_ExportingTheModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
