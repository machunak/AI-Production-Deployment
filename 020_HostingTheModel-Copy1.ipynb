{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4Jdl9ZbDoQI"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"헤더\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jTONVvoDoQL"
   },
   "source": [
    "# 2.0 모델 호스팅\n",
    "\n",
    "이 노트북에서는 배포 성능을 개선하기 위해 Triton Server를 최적화하는 전략을 배웁니다.\n",
    "\n",
    "\n",
    "**[2.1 동시 모델 실행](#2.1-동시-모델-실행)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 연습: 사용 고려 사항](#2.1.1-연습:-사용-고려-사항)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 구현](#2.1.2-구현)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 연습: 여러 인스턴스 그룹 구성](#2.1.3-연습:-여러-인스턴스-그룹-구성)<br>\n",
    "**[2.2 스케줄링 전략](#2.2-스케줄링-전략)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 스테이트리스 추론](#2.2.1-스테이트리스-추론)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 스테이트풀 추론](#2.2.2-스테이트풀-추론)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 파이프라인/앙상블](#2.2.3-파이프라인/앙상블)<br>\n",
    "**[2.3 동적 배치](#2.3-동적-배치)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 연습: 동적 배치 구현](#2.3.1-연습:-동적-배치-구현)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7ZnagCPDoQM"
   },
   "source": [
    "지금까지 고객 요청을 서버에 도착한 순서대로 순차적으로 실행했으며 서버에 도착하는 모든 요청에 크기 8의 정적 배치를 사용했습니다. 그 결과 GPU가 거의 활용되지 않은 상태로 남았을 뿐 아니라 서버에서 수신한 응답의 지연 시간에 큰 영향을 미쳤습니다. 이는 드문 상황이 아닙니다. 대량의 데이터를 배치로 처리하는 애플리케이션을 개발하지 않으면 사용자 애플리케이션에서 개별 추론 요청을 보낼 가능성이 높으므로 저활용 상태가 훨씬 더 심해질 수 있습니다. 이전 노트북에서 살펴본 것처럼 모델 최적화는 모델 실행을 가속화하는 데 상당한 도움이 됩니다.  그러나 서빙이 단순하게 구현될 경우 추론 워크로드의 특성으로 인해 GPU 저활용 상태로 이어진다는 사실은 변하지 않습니다.\n",
    "\n",
    "NVIDIA Triton가 같은 추론 서버는 GPU 활용률을 높이고 요청 지연 시간을 개선할 수 있는 다양한 기능을 구현합니다. 이 수업에서는 다음 세 가지를 설명하겠습니다.<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html#concurrent-model-execution\">동시 모델 실행</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#scheduling-and-batching\">스케줄링</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#dynamic-batcher\">동적 배치</a> <br/>\n",
    "\n",
    "\n",
    "모델 추론 성능 개선에 도움이 될 수 있는 메커니즘 및 구성에 대한 자세한 내용은 <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\">Triton 문서</a> 및 해당 <a href=\"https://github.com/NVIDIA/triton-inference-server\">소스 코드</a>를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS5yluONDoQN"
   },
   "source": [
    "# 2.1 동시 모델 실행\n",
    "Triton 아키텍처를 사용하면 여러 모델 및/또는 동일한 모델의 여러 인스턴스를 단일 GPU에서 병렬로 실행할 수 있습니다. 다음 그림은 두 가지 모델 `model0` 및 `model1`을 실행한 예를 보여줍니다. Triton이 현재 어떤 요청도 처리하지 않고 있다고 가정할 때 각 모델에 대해 하나씩 2개의 요청이 동시에 도착할 경우 Triton이 즉시 두 요청을 GPU에 예약하고 GPU의 하드웨어 스케줄러가 두 연산을 병렬로 처리하기 시작합니다. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### 기본 동작\n",
    "\n",
    "기본적으로 동일한 모델에 대해 여러 개의 요청이 동시에 도착할 경우 Triton은 다음 그림과 같이 GPU에서 한 번에 한 개의 요청만 예약하여 요청 실행을 직렬화합니다.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton은 각 모델이 모델에서 허용해야 하는 병렬 실행 수를 지정할 수 있는 인스턴스-그룹 기능을 제공합니다. 그렇게 활성화된 병렬 실행을 각각 *실행 인스턴스*라고 합니다. 기본적으로 Triton은 각 모델에 단일 실행 인스턴스를 제공하며, 이는 위 그림에 표시된 것처럼 한 번에 모델의 하나의 실행만 진행할 수 있음을 의미합니다. \n",
    "\n",
    "#### 인스턴스 그룹\n",
    "*인스턴스-그룹* 설정을 사용하여 모델에 대한 실행 인스턴스의 수를 늘릴 수 있습니다. 다음 그림은 `model1`이 3개의 실행 인스턴스를 허용하도록 구성된 경우 모델 실행을 보여줍니다. 그림과 같이 처음 3개의 `model1` 추론 요청이 GPU에서 즉시 병렬로 실행됩니다. 네 번째 `model1` 추론 요청은 처음 3개의 실행 중 하나가 완료될 때까지 기다려야 시작됩니다.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5dYN8L2DoQN"
   },
   "source": [
    "## 2.1.1 연습: 사용 고려 사항\n",
    "\n",
    "대부분의 모델의 경우 가장 큰 성능 개선을 제공하는 Triton 기능은 *동적 배치*입니다. 여러 인스턴스 실행을 설정하는 것과 비교할 때 동적 배치의 주요 이점은 다음과 같습니다.\n",
    "- 모델 매개변수 저장을 위한 오버헤드 없음\n",
    "- GPU 메모리에서 모델 매개변수 가져오기와 관련된 오버헤드 없음\n",
    "- GPU 리소스를 더 잘 활용할 수 있음\n",
    "\n",
    "여러 모델 실행의 구성을 살펴보기 전에 단일 인스턴스를 사용해 모델을 다시 실행하고 GPU의 리소스 활용률을 관찰해 보겠습니다. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfHSxGKBDoQN"
   },
   "source": [
    "#### 연습 단계\n",
    "1. JupyterLab 시작 페이지에서 터미널 창을 시작합니다.  새 시작 페이지를 열어야 할 경우 왼쪽 사이드바 메뉴에서 '+' 아이콘을 클릭합니다. 그런 다음 드래그 앤 드롭 작업을 사용하여 터미널을 더 잘 볼 수 있도록 하위 창 구성으로 이동할 수 있습니다.\n",
    "2. 성능 도구를 실행하기 전에 터미널에서 다음 명령을 실행합니다.<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    다음과 같은 출력이 표시되어야 합니다.\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. 이전 노트북에서 사용한 것과 동일한 벤치마크를 실행하되 배치 크기를 1로 줄이고 <code>nvidia-smi</code> 출력을 다시 관찰합니다.  메모리 소비 및 GPU 활용률에 특히 주목하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OR8nv56YDoQO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H4-x-LoTDoQO"
   },
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion = \"1\"\n",
    "precision = \"fp32\"\n",
    "batchSize = \"8\"\n",
    "maxLatency = \"500\"\n",
    "maxClientThreads = \"10\"\n",
    "maxConcurrency = \"2\"\n",
    "dockerBridge = \"host\"\n",
    "resultsFolderName = \"1\"\n",
    "profilingData = \"utilities/profiling_data_int64\"\n",
    "measurement_request_count = 50\n",
    "percentile_stability = 85\n",
    "stability_percentage = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ECBmgrSyDoQO",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 55.9789 infer/sec. p85 latency: 17721 usec\n",
      "  Pass [2] throughput: 56.9691 infer/sec. p85 latency: 17721 usec\n",
      "  Pass [3] throughput: 55.9739 infer/sec. p85 latency: 17736 usec\n",
      "  Client: \n",
      "    Request count: 169\n",
      "    Throughput: 56.3073 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 56.64 infer/sec\n",
      "    p50 latency: 17673 usec\n",
      "    p85 latency: 17724 usec\n",
      "    p90 latency: 17737 usec\n",
      "    p95 latency: 17758 usec\n",
      "    p99 latency: 17820 usec\n",
      "    Avg gRPC time: 17675 usec (marshal 2 usec + response wait 17672 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 169\n",
      "    Execution count: 169\n",
      "    Successful request count: 169\n",
      "    Avg request latency: 17553 usec (overhead 67 usec + queue 21 usec + compute input 22 usec + compute infer 17436 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 56.9813 infer/sec. p85 latency: 35197 usec\n",
      "  Pass [2] throughput: 56.9752 infer/sec. p85 latency: 35185 usec\n",
      "  Pass [3] throughput: 56.9748 infer/sec. p85 latency: 35159 usec\n",
      "  Client: \n",
      "    Request count: 171\n",
      "    Throughput: 56.9771 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 57.31 infer/sec\n",
      "    p50 latency: 35143 usec\n",
      "    p85 latency: 35185 usec\n",
      "    p90 latency: 35195 usec\n",
      "    p95 latency: 35210 usec\n",
      "    p99 latency: 35234 usec\n",
      "    Avg gRPC time: 34952 usec (marshal 2 usec + response wait 34949 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 171\n",
      "    Execution count: 171\n",
      "    Successful request count: 171\n",
      "    Avg request latency: 34793 usec (overhead 63 usec + queue 17233 usec + compute input 21 usec + compute infer 17468 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 56.9759 infer/sec. p85 latency: 52706 usec\n",
      "  Pass [2] throughput: 56.9662 infer/sec. p85 latency: 52709 usec\n",
      "  Pass [3] throughput: 56.9751 infer/sec. p85 latency: 52728 usec\n",
      "  Client: \n",
      "    Request count: 171\n",
      "    Throughput: 56.9724 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 57.31 infer/sec\n",
      "    p50 latency: 52659 usec\n",
      "    p85 latency: 52707 usec\n",
      "    p90 latency: 52723 usec\n",
      "    p95 latency: 52731 usec\n",
      "    p99 latency: 52759 usec\n",
      "    Avg gRPC time: 52383 usec (marshal 2 usec + response wait 52380 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 171\n",
      "    Execution count: 171\n",
      "    Successful request count: 171\n",
      "    Avg request latency: 52172 usec (overhead 62 usec + queue 34628 usec + compute input 21 usec + compute infer 17453 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 56.9781 infer/sec. p85 latency: 70230 usec\n",
      "  Pass [2] throughput: 56.9764 infer/sec. p85 latency: 70330 usec\n",
      "  Pass [3] throughput: 56.9768 infer/sec. p85 latency: 70302 usec\n",
      "  Client: \n",
      "    Request count: 171\n",
      "    Throughput: 56.9771 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 57.31 infer/sec\n",
      "    p50 latency: 70217 usec\n",
      "    p85 latency: 70304 usec\n",
      "    p90 latency: 70320 usec\n",
      "    p95 latency: 70357 usec\n",
      "    p99 latency: 70373 usec\n",
      "    Avg gRPC time: 69845 usec (marshal 2 usec + response wait 69842 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 171\n",
      "    Execution count: 171\n",
      "    Successful request count: 171\n",
      "    Avg request latency: 69651 usec (overhead 62 usec + queue 52104 usec + compute input 21 usec + compute infer 17457 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 56.9723 infer/sec. p85 latency: 87798 usec\n",
      "  Pass [2] throughput: 56.9747 infer/sec. p85 latency: 87905 usec\n",
      "  Pass [3] throughput: 56.9735 infer/sec. p85 latency: 87973 usec\n",
      "  Client: \n",
      "    Request count: 171\n",
      "    Throughput: 56.9735 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 57.31 infer/sec\n",
      "    p50 latency: 87841 usec\n",
      "    p85 latency: 87929 usec\n",
      "    p90 latency: 87939 usec\n",
      "    p95 latency: 87973 usec\n",
      "    p99 latency: 87996 usec\n",
      "    Avg gRPC time: 87345 usec (marshal 2 usec + response wait 87342 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 171\n",
      "    Execution count: 171\n",
      "    Successful request count: 171\n",
      "    Avg request latency: 87128 usec (overhead 62 usec + queue 69572 usec + compute input 22 usec + compute infer 17464 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 56.9798 infer/sec. p85 latency: 105434 usec\n",
      "  Pass [2] throughput: 56.978 infer/sec. p85 latency: 105616 usec\n",
      "  Pass [3] throughput: 56.9789 infer/sec. p85 latency: 105662 usec\n",
      "  Client: \n",
      "    Request count: 171\n",
      "    Throughput: 56.9789 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 57.31 infer/sec\n",
      "    p50 latency: 105518 usec\n",
      "    p85 latency: 105622 usec\n",
      "    p90 latency: 105637 usec\n",
      "    p95 latency: 105664 usec\n",
      "    p99 latency: 105708 usec\n",
      "    Avg gRPC time: 104912 usec (marshal 3 usec + response wait 104908 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 171\n",
      "    Execution count: 171\n",
      "    Successful request count: 171\n",
      "    Avg request latency: 104695 usec (overhead 63 usec + queue 87124 usec + compute input 24 usec + compute infer 17475 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 56.9823 infer/sec. p85 latency: 123222 usec\n",
      "  Pass [2] throughput: 55.977 infer/sec. p85 latency: 123290 usec\n",
      "  Pass [3] throughput: 56.972 infer/sec. p85 latency: 123155 usec\n",
      "  Client: \n",
      "    Request count: 170\n",
      "    Throughput: 56.6438 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 56.98 infer/sec\n",
      "    p50 latency: 123147 usec\n",
      "    p85 latency: 123221 usec\n",
      "    p90 latency: 123244 usec\n",
      "    p95 latency: 123290 usec\n",
      "    p99 latency: 123507 usec\n",
      "    Avg gRPC time: 122507 usec (marshal 3 usec + response wait 122503 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 170\n",
      "    Execution count: 170\n",
      "    Successful request count: 170\n",
      "    Avg request latency: 122271 usec (overhead 64 usec + queue 104688 usec + compute input 25 usec + compute infer 17485 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 56.9803 infer/sec. p85 latency: 140775 usec\n",
      "  Pass [2] throughput: 56.9746 infer/sec. p85 latency: 140718 usec\n",
      "  Pass [3] throughput: 56.969 infer/sec. p85 latency: 140914 usec\n",
      "  Client: \n",
      "    Request count: 171\n",
      "    Throughput: 56.9747 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 57.31 infer/sec\n",
      "    p50 latency: 140684 usec\n",
      "    p85 latency: 140843 usec\n",
      "    p90 latency: 140875 usec\n",
      "    p95 latency: 140917 usec\n",
      "    p99 latency: 140954 usec\n",
      "    Avg gRPC time: 139904 usec (marshal 3 usec + response wait 139900 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 171\n",
      "    Execution count: 171\n",
      "    Successful request count: 171\n",
      "    Avg request latency: 139670 usec (overhead 63 usec + queue 122092 usec + compute input 24 usec + compute infer 17482 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 56.9809 infer/sec. p85 latency: 158532 usec\n",
      "  Pass [2] throughput: 56.975 infer/sec. p85 latency: 158361 usec\n",
      "  Pass [3] throughput: 56.9754 infer/sec. p85 latency: 158398 usec\n",
      "  Client: \n",
      "    Request count: 171\n",
      "    Throughput: 56.9771 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 57.31 infer/sec\n",
      "    p50 latency: 158331 usec\n",
      "    p85 latency: 158418 usec\n",
      "    p90 latency: 158441 usec\n",
      "    p95 latency: 158532 usec\n",
      "    p99 latency: 158562 usec\n",
      "    Avg gRPC time: 157468 usec (marshal 3 usec + response wait 157464 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 171\n",
      "    Execution count: 171\n",
      "    Successful request count: 171\n",
      "    Avg request latency: 157233 usec (overhead 63 usec + queue 139651 usec + compute input 25 usec + compute infer 17485 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 55.9814 infer/sec. p85 latency: 175848 usec\n",
      "  Pass [2] throughput: 56.9687 infer/sec. p85 latency: 175902 usec\n",
      "  Pass [3] throughput: 56.973 infer/sec. p85 latency: 175926 usec\n",
      "  Client: \n",
      "    Request count: 170\n",
      "    Throughput: 56.6411 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 56.97 infer/sec\n",
      "    p50 latency: 175821 usec\n",
      "    p85 latency: 175894 usec\n",
      "    p90 latency: 175905 usec\n",
      "    p95 latency: 175942 usec\n",
      "    p99 latency: 175991 usec\n",
      "    Avg gRPC time: 174876 usec (marshal 3 usec + response wait 174872 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 170\n",
      "    Execution count: 170\n",
      "    Successful request count: 170\n",
      "    Avg request latency: 174631 usec (overhead 62 usec + queue 157060 usec + compute input 25 usec + compute infer 17475 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 56.3073 infer/sec, latency 17724 usec\n",
      "Concurrency: 2, throughput: 56.9771 infer/sec, latency 35185 usec\n",
      "Concurrency: 3, throughput: 56.9724 infer/sec, latency 52707 usec\n",
      "Concurrency: 4, throughput: 56.9771 infer/sec, latency 70304 usec\n",
      "Concurrency: 5, throughput: 56.9735 infer/sec, latency 87929 usec\n",
      "Concurrency: 6, throughput: 56.9789 infer/sec, latency 105622 usec\n",
      "Concurrency: 7, throughput: 56.6438 infer/sec, latency 123221 usec\n",
      "Concurrency: 8, throughput: 56.9747 infer/sec, latency 140843 usec\n",
      "Concurrency: 9, throughput: 56.9771 infer/sec, latency 158418 usec\n",
      "Concurrency: 10, throughput: 56.6411 infer/sec, latency 175894 usec\n",
      "CPU times: user 280 ms, sys: 131 ms, total: 412 ms\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YySWuCa5DoQP"
   },
   "source": [
    "다음과 비슷한 활용률이 관찰되었을 것입니다.<br/>\n",
    "<img src=\"images/NVIDIASMI2.png\" width=800/><br/>\n",
    "\n",
    "인스턴스 그룹의 수를 늘린 결과로 큰 속도 증가가 관찰될 것이라고 생각하십니까?<br>\n",
    "강사와 논의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edGU6odWDoQP"
   },
   "source": [
    "## 2.1.2 구현\n",
    "동시 실행을 활성화하는 방법과 이것이 모델 성능에 미치는 영향을 살펴보겠습니다. 다음 코드 셀을 실행하여 모델을 ONNX 형식으로 내보냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9drf-RuDDoQP"
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h6nOeh5lDoQP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.32584524154663086 seconds\n",
      "time of error check of onnx model:  10.106379508972168 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zLEl4ZYADoQQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K Mar  4 06:19 .\n",
      "drwxr-xr-x 3 root root 4.0K Mar  4 06:18 ..\n",
      "drwxr-xr-x 2 root root 4.0K Mar  4 06:18 1\n",
      "-rw-r--r-- 1 root root  569 Mar  4 06:19 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhY5Vdv1DoQQ"
   },
   "source": [
    "## 2.1.3 연습: 여러 인스턴스 그룹 구성\n",
    "여러 인스턴스를 지정하려면 \"config.pbtxt\" 구성 파일에 있는 `instance_group` 섹션에서 \"개수\" 값을 '1'에서 더 큰 수로 변경해야 합니다. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### 연습 단계:\n",
    "1. 방금 만든 `bertQA-onnx-conexec` 배포에서 [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt)를 수정하여 BERT 기반 질문 답변 모델의 두 인스턴스를 지정합니다. 파일의 끝에서 기본 instance_group 블록을 찾아야 합니다. 개수 변수를 1에서 2로 변경합니다.  (필요에 따라 [솔루션](solutions/ex-2-1-3_config.pbtxt) 참조)\n",
    "2. 공정하게 비교하려면 `optimization` 블록 내에 `execution_accelerators` 블록을 추가해 TensorRT도 활성화합니다.\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. 변경 사항을 저장한 후(주 메뉴: 파일 -> 파일 저장), 다음 명령을 실행하여 모델을 Triton으로 이동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# quick fix!\n",
    "!cp solutions/ex-2-1-3_config.pbtxt candidatemodels/bertQA-onnx-conexec/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZLJqZEzWDoQQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat './candidatemodels/bertQA-onnx-conexec': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB6M2oI_DoQQ"
   },
   "source": [
    "4. 모델에 대해 표준 스트레스 테스트를 실행합니다. 이를 단일 인스턴스 실행과 비교하십시오.<br>\n",
    "   처리량이 변경되었습니까?<br>\n",
    "   지연 시간이 변경되었습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "obD0Wg2xDoQR",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 1.1772 infer/sec. p85 latency: 6181 usec\n",
      "  Pass [2] throughput: 163.923 infer/sec. p85 latency: 6181 usec\n",
      "  Pass [3] throughput: 163.926 infer/sec. p85 latency: 6149 usec\n",
      "  Pass [4] throughput: 163.925 infer/sec. p85 latency: 6184 usec\n",
      "  Client: \n",
      "    Request count: 492\n",
      "    Throughput: 163.925 infer/sec\n",
      "    Avg client overhead: 0.04%\n",
      "    Avg send request rate: 163.92 infer/sec\n",
      "    p50 latency: 6085 usec\n",
      "    p85 latency: 6171 usec\n",
      "    p90 latency: 6189 usec\n",
      "    p95 latency: 6215 usec\n",
      "    p99 latency: 6247 usec\n",
      "    Avg gRPC time: 6098 usec (marshal 2 usec + response wait 6095 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 492\n",
      "    Execution count: 492\n",
      "    Successful request count: 492\n",
      "    Avg request latency: 5976 usec (overhead 68 usec + queue 36 usec + compute input 27 usec + compute infer 5838 usec + compute output 6 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 179.92 infer/sec. p85 latency: 11691 usec\n",
      "  Pass [2] throughput: 182.881 infer/sec. p85 latency: 11660 usec\n",
      "  Pass [3] throughput: 181.916 infer/sec. p85 latency: 11712 usec\n",
      "  Client: \n",
      "    Request count: 545\n",
      "    Throughput: 181.572 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 181.91 infer/sec\n",
      "    p50 latency: 11065 usec\n",
      "    p85 latency: 11691 usec\n",
      "    p90 latency: 11935 usec\n",
      "    p95 latency: 12247 usec\n",
      "    p99 latency: 12998 usec\n",
      "    Avg gRPC time: 10996 usec (marshal 2 usec + response wait 10993 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 545\n",
      "    Execution count: 545\n",
      "    Successful request count: 545\n",
      "    Avg request latency: 10853 usec (overhead 63 usec + queue 22 usec + compute input 42 usec + compute infer 10719 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 189.941 infer/sec. p85 latency: 19806 usec\n",
      "  Pass [2] throughput: 187.919 infer/sec. p85 latency: 19944 usec\n",
      "  Pass [3] throughput: 187.916 infer/sec. p85 latency: 19844 usec\n",
      "  Client: \n",
      "    Request count: 566\n",
      "    Throughput: 188.592 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 188.93 infer/sec\n",
      "    p50 latency: 15763 usec\n",
      "    p85 latency: 19842 usec\n",
      "    p90 latency: 19994 usec\n",
      "    p95 latency: 20155 usec\n",
      "    p99 latency: 20502 usec\n",
      "    Avg gRPC time: 15866 usec (marshal 2 usec + response wait 15863 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 566\n",
      "    Execution count: 566\n",
      "    Successful request count: 566\n",
      "    Avg request latency: 15686 usec (overhead 63 usec + queue 5097 usec + compute input 42 usec + compute infer 10476 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 189.941 infer/sec. p85 latency: 21646 usec\n",
      "  Pass [2] throughput: 188.911 infer/sec. p85 latency: 21603 usec\n",
      "  Pass [3] throughput: 188.905 infer/sec. p85 latency: 21805 usec\n",
      "  Client: \n",
      "    Request count: 568\n",
      "    Throughput: 189.252 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 189.59 infer/sec\n",
      "    p50 latency: 21019 usec\n",
      "    p85 latency: 21657 usec\n",
      "    p90 latency: 21847 usec\n",
      "    p95 latency: 22135 usec\n",
      "    p99 latency: 22680 usec\n",
      "    Avg gRPC time: 21086 usec (marshal 2 usec + response wait 21083 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 568\n",
      "    Execution count: 568\n",
      "    Successful request count: 568\n",
      "    Avg request latency: 20920 usec (overhead 63 usec + queue 10366 usec + compute input 41 usec + compute infer 10443 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 189.917 infer/sec. p85 latency: 30411 usec\n",
      "  Pass [2] throughput: 186.911 infer/sec. p85 latency: 30244 usec\n",
      "  Pass [3] throughput: 188.906 infer/sec. p85 latency: 30209 usec\n",
      "  Client: \n",
      "    Request count: 566\n",
      "    Throughput: 188.578 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 188.91 infer/sec\n",
      "    p50 latency: 26865 usec\n",
      "    p85 latency: 30330 usec\n",
      "    p90 latency: 30499 usec\n",
      "    p95 latency: 30729 usec\n",
      "    p99 latency: 31092 usec\n",
      "    Avg gRPC time: 26508 usec (marshal 2 usec + response wait 26505 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 566\n",
      "    Execution count: 566\n",
      "    Successful request count: 566\n",
      "    Avg request latency: 26330 usec (overhead 63 usec + queue 15715 usec + compute input 43 usec + compute infer 10502 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 186.928 infer/sec. p85 latency: 32926 usec\n",
      "  Pass [2] throughput: 185.915 infer/sec. p85 latency: 33204 usec\n",
      "  Pass [3] throughput: 187.912 infer/sec. p85 latency: 32745 usec\n",
      "  Client: \n",
      "    Request count: 561\n",
      "    Throughput: 186.918 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 187.25 infer/sec\n",
      "    p50 latency: 31967 usec\n",
      "    p85 latency: 32975 usec\n",
      "    p90 latency: 33210 usec\n",
      "    p95 latency: 33599 usec\n",
      "    p99 latency: 34223 usec\n",
      "    Avg gRPC time: 32053 usec (marshal 2 usec + response wait 32050 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 561\n",
      "    Execution count: 561\n",
      "    Successful request count: 561\n",
      "    Avg request latency: 31863 usec (overhead 64 usec + queue 21170 usec + compute input 46 usec + compute infer 10576 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 185.93 infer/sec. p85 latency: 40543 usec\n",
      "  Pass [2] throughput: 184.919 infer/sec. p85 latency: 40464 usec\n",
      "  Pass [3] throughput: 184.918 infer/sec. p85 latency: 40149 usec\n",
      "  Client: \n",
      "    Request count: 556\n",
      "    Throughput: 185.256 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 185.59 infer/sec\n",
      "    p50 latency: 37914 usec\n",
      "    p85 latency: 40455 usec\n",
      "    p90 latency: 40735 usec\n",
      "    p95 latency: 41076 usec\n",
      "    p99 latency: 41664 usec\n",
      "    Avg gRPC time: 37713 usec (marshal 3 usec + response wait 37709 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 556\n",
      "    Execution count: 556\n",
      "    Successful request count: 556\n",
      "    Avg request latency: 37511 usec (overhead 64 usec + queue 26727 usec + compute input 46 usec + compute infer 10666 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 184.931 infer/sec. p85 latency: 44234 usec\n",
      "  Pass [2] throughput: 184.911 infer/sec. p85 latency: 44362 usec\n",
      "  Pass [3] throughput: 184.914 infer/sec. p85 latency: 44583 usec\n",
      "  Client: \n",
      "    Request count: 555\n",
      "    Throughput: 184.919 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 185.25 infer/sec\n",
      "    p50 latency: 43182 usec\n",
      "    p85 latency: 44456 usec\n",
      "    p90 latency: 44703 usec\n",
      "    p95 latency: 45333 usec\n",
      "    p99 latency: 45796 usec\n",
      "    Avg gRPC time: 43152 usec (marshal 3 usec + response wait 43148 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 555\n",
      "    Execution count: 555\n",
      "    Successful request count: 555\n",
      "    Avg request latency: 42957 usec (overhead 63 usec + queue 32160 usec + compute input 46 usec + compute infer 10680 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 184.925 infer/sec. p85 latency: 51464 usec\n",
      "  Pass [2] throughput: 185.915 infer/sec. p85 latency: 51979 usec\n",
      "  Pass [3] throughput: 186.92 infer/sec. p85 latency: 51469 usec\n",
      "  Client: \n",
      "    Request count: 558\n",
      "    Throughput: 185.92 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 186.25 infer/sec\n",
      "    p50 latency: 48494 usec\n",
      "    p85 latency: 51661 usec\n",
      "    p90 latency: 51959 usec\n",
      "    p95 latency: 52346 usec\n",
      "    p99 latency: 52756 usec\n",
      "    Avg gRPC time: 48315 usec (marshal 3 usec + response wait 48311 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 558\n",
      "    Execution count: 558\n",
      "    Successful request count: 558\n",
      "    Avg request latency: 48116 usec (overhead 64 usec + queue 37368 usec + compute input 47 usec + compute infer 10630 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 187.929 infer/sec. p85 latency: 54149 usec\n",
      "  Pass [2] throughput: 187.889 infer/sec. p85 latency: 53877 usec\n",
      "  Pass [3] throughput: 186.891 infer/sec. p85 latency: 54275 usec\n",
      "  Client: \n",
      "    Request count: 563\n",
      "    Throughput: 187.569 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 187.90 infer/sec\n",
      "    p50 latency: 53139 usec\n",
      "    p85 latency: 54149 usec\n",
      "    p90 latency: 54370 usec\n",
      "    p95 latency: 54683 usec\n",
      "    p99 latency: 55504 usec\n",
      "    Avg gRPC time: 53157 usec (marshal 3 usec + response wait 53153 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 563\n",
      "    Execution count: 563\n",
      "    Successful request count: 563\n",
      "    Avg request latency: 52960 usec (overhead 64 usec + queue 42321 usec + compute input 44 usec + compute infer 10523 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 163.925 infer/sec, latency 6171 usec\n",
      "Concurrency: 2, throughput: 181.572 infer/sec, latency 11691 usec\n",
      "Concurrency: 3, throughput: 188.592 infer/sec, latency 19842 usec\n",
      "Concurrency: 4, throughput: 189.252 infer/sec, latency 21657 usec\n",
      "Concurrency: 5, throughput: 188.578 infer/sec, latency 30330 usec\n",
      "Concurrency: 6, throughput: 186.918 infer/sec, latency 32975 usec\n",
      "Concurrency: 7, throughput: 185.256 infer/sec, latency 40455 usec\n",
      "Concurrency: 8, throughput: 184.919 infer/sec, latency 44456 usec\n",
      "Concurrency: 9, throughput: 185.92 infer/sec, latency 51661 usec\n",
      "Concurrency: 10, throughput: 187.569 infer/sec, latency 54149 usec\n",
      "CPU times: user 1.25 s, sys: 436 ms, total: 1.69 s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-conexec\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhM97UTSDoQR"
   },
   "source": [
    "계속하기 전에 Triton 모델 리포지토리에서 일부 모델을 이동하여 GPU 메모리를 확보하겠습니다.  다음 3개 모델을 제거한 후 `bertQA-torchscript` 모델만 남아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "I-qT4A1gDoQR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertQA-torchscript\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EuCm-_JDoQR"
   },
   "source": [
    "# 2.2 스케줄링 전략\n",
    "Triton은 개별 추론 요청에서 입력 배치를 지정할 수 있도록 허용하여 배치 추론을 지원합니다. 입력 배치 추론은 동시에 수행되며, 이는 추론 처리량을 크게 늘릴 수 있으므로 GPU에 특히 중요합니다. 많은 사용 사례에서 개별 추론 요청은 배치 처리되지 않으므로 배치의 처리량 이점을 얻을 수 없습니다. <br/>\n",
    "\n",
    "추론 서버에는 여러 가지 다양한 모델 유형과 사용 사례를 지원하는 여러 스케줄링 및 배치 알고리즘이 포함되어 있습니다. 스케줄러/배처 선택은 여러 가지 요인에 따라 결정되며 핵심 요인은 다음과 같습니다.\n",
    "- 추론 워크로드의 스테이트풀/스테이트리스 특성\n",
    "- 애플리케이션이 격리되어 처리되는 모델로 구성되어 있는지 여부 또는 더 복잡한 파이프라인/앙상블이 사용되는지 여부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZdiJedJDoQS"
   },
   "source": [
    "## 2.2.1 스테이트리스 추론\n",
    "\n",
    "이 수업에서처럼 스테이트리스 추론을 다룰 경우 스케줄링과 관련해서 두 가지 주요 옵션을 사용할 수 있습니다. 첫 번째 옵션은 추론에 할당된 모든 인스턴스에 요청을 분산하는 기본 스케줄러입니다. 이는 추론 워크로드의 구조가 잘 이해되고 추론이 규칙적인 배치 크기 및 시간 간격으로 발생하는 경우에 선호되는 옵션입니다.\n",
    "\n",
    "두 번째 옵션은 개별 요청을 결합하여 기본 배처와 유사하게 대규모 배치를 인스턴스 전체에 분산하는 동적 배치입니다. 이 특별 옵션은 수업의 다음 섹션에서 설명하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fb3lad8DoQS"
   },
   "source": [
    "## 2.2.2 스테이트풀 추론\n",
    "\n",
    "스테이트풀 모델(또는 스테이트풀 맞춤형 백엔드)은 추론 요청 간에 상태를 유지합니다. 이 모델은 여러 추론 요청이 동일한 모델 인스턴스로 라우팅되어야 하는 추론 시퀀스를 형성하므로 모델에서 유지하는 상태가 올바르게 업데이트됩니다. 뿐만 아니라 이 모델은 Triton이 예를 들어 시퀀스 시작을 나타내는 제어 신호를 제공하도록 요구할 수도 있습니다.\n",
    "\n",
    "시퀀스 배처는 동일한 모델 인스턴스로 라우팅되는 시퀀스를 배치 처리하는 방식을 결정할 때 2개의 스케줄링 전략 중 하나를 사용할 수 있습니다. 이러한 전략은 다이렉트(Direct) 스케줄링 전략과 올디스트(Oldest) 스케줄링 전략입니다.\n",
    "\n",
    "다이렉트 스케줄링 전략을 사용하면 시퀀스 배처가 시퀀스의 모든 추론 요청이 동일한 모델 인스턴스로 라우팅되게 할 뿐 아니라 각 시퀀스가 모델 인스턴스 내의 전용 배치 슬롯으로 라우팅되게 합니다. 이 전략은 모델이 각 배치 슬롯의 상태를 유지할 경우에 필요하며, 주어진 시퀀스의 모든 추론 요청이 동일한 슬롯으로 라우팅되므로 상태가 올바르게 업데이트됩니다.\n",
    "\n",
    "올디스트 스케줄링 전략을 사용하면 시퀀스 배처가 시퀀스의 모든 추론 요청이 동일한 모델 인스턴스로 라우팅된 다음 동적 배처를 사용하여 다양한 시퀀스의 여러 추론을 함께 추론을 수행하는 배치로 한꺼번에 처리하도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh72tIdxDoQS"
   },
   "source": [
    "## 2.2.3 파이프라인/앙상블\n",
    "\n",
    "앙상블 모델은 하나 이상의 모델의 파이프라인과 그러한 모델 간의 입력 및 출력 텐서 연결을 나타냅니다. 앙상블 모델은 \"데이터 사전 처리 -> 추론 -> 데이터 사후 처리\"와 같은 여러 모델이 관련된 절차를 캡슐화하는 데 사용되도록 고안되었습니다. 이 목적으로 앙상블 모델을 사용할 경우 중간 텐서를 전송하는 데 따른 오버헤드를 피하고 Triton으로 전송되어야 하는 요청의 수를 최소화할 수 있습니다. 앙상블 파이프라인의 예가 아래에 나와 있습니다. <br/>\n",
    "\n",
    "<img src=\"images/ensemble_example0.png\"/>\n",
    "\n",
    "앙상블 내에서 모델이 사용하는 스케줄러에 관계없이 앙상블 스케줄러가 앙상블 모델에 사용되어야 합니다. 앙상블 스케줄러와 관련해서 앙상블 모델은 실제 모델이 아닙니다. 대신, 앙상블 모델은 앙상블 내의 모델 간의 데이터 흐름을 단계로 지정합니다. 스케줄러는 각 단계에서 출력 텐서를 수집하고 이를 지정에 따라 다른 단계의 입력 텐서로 제공합니다. 그럼에도 불구하고 앙상블 모델은 여전히 외부에서 봤을 때 단일 모델로 보여집니다.\n",
    "\n",
    "Triton 스케줄링에 대한 자세한 내용은 <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html#stateless-models\">문서의 다음 섹션</a>에서 확인할 수 있습니다. 이 수업에서는 Triton의 가장 강력한 기능 중 하나인 *동적 배치*를 더 중점적으로 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HddH44uFDoQS"
   },
   "source": [
    "# 2.3 동적 배치\n",
    "동적 배치는 Triton의 한 기능으로, 배치가 동적으로 생성되도록 서버에서 추론 요청을 결합할 수 있어 결과적으로 처리량이 늘어납니다.\n",
    "\n",
    "모델 인스턴스를 추론에 사용할 수 있게 될 경우 동적 배처가 스케줄러에서 사용할 수 있는 요청에서 배치를 만들려고 시도합니다. 요청이 수신된 순서로 배치에 추가됩니다. 동적 배처가 선호하는 크기의 배치를 형성할 수 있는 경우 가능한 한 가장 큰 선호하는 크기의 배치를 만들고 추론을 위해 전송합니다. 동적 배처가 선호하는 크기의 배치를 형성할 수 없는 경우에는 모델에서 허용하는 최대 배치 크기보다 작은 가능한 한 가장 큰 배치를 전송합니다. \n",
    "\n",
    "동적 배처는 다른 요청이 동적 배치에 참여할 수 있도록 스케줄러에서 제한된 시간 동안 요청을 지연시킬 수 있게 구성할 수 있습니다. 예를 들어, 다음 구성에서는 요청에 대해 최대 100마이크로초의 지연 시간을 설정합니다.\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP1wcWt8DoQU"
   },
   "source": [
    "## 2.3.1 연습: 동적 배치 구현\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8h8jBFhDoQU"
   },
   "source": [
    "다시 ONNX 모델을 내보내는 것으로 시작해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0FMa0ouWDoQW"
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kO0z-wfFDoQW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.3257179260253906 seconds\n",
      "time of error check of onnx model:  9.569658279418945 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AaJjWZ5DoQX"
   },
   "source": [
    "#### 연습 단계\n",
    "1. 예제 스니펫을 사용하여 동적 배치의 [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt)를 수정합니다. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. 최적화 블록에서 TensorRT를 활성화합니다.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. 저장되면 모델을 Triton 모델 리포지토리로 이동하고 다음 셀을 실행하여 성능 유틸리티를 실행합니다. (필요한 경우 [솔루션](solutions/ex-2-3-1_config.pbtxt) 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick fix!\n",
    "!cp solutions/ex-2-3-1_config.pbtxt candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aJCbme5gDoQX"
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 99\n",
      "    Throughput: 197.935 infer/sec\n",
      "    p50 latency: 40704 usec\n",
      "    p85 latency: 40775 usec\n",
      "    p90 latency: 40791 usec\n",
      "    p95 latency: 40806 usec\n",
      "    p99 latency: 40857 usec\n",
      "    Avg gRPC time: 40703 usec ((un)marshal request/response 6 usec + response wait 40697 usec)\n",
      "  Server: \n",
      "    Inference count: 792\n",
      "    Execution count: 99\n",
      "    Successful request count: 99\n",
      "    Avg request latency: 40484 usec (overhead 70 usec + queue 56 usec + compute input 30 usec + compute infer 40318 usec + compute output 10 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 197.935 infer/sec, latency 40775 usec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 4\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 167.928 infer/sec\n",
      "    p50 latency: 23806 usec\n",
      "    p85 latency: 23888 usec\n",
      "    p90 latency: 23909 usec\n",
      "    p95 latency: 23943 usec\n",
      "    p99 latency: 23985 usec\n",
      "    Avg gRPC time: 23812 usec ((un)marshal request/response 5 usec + response wait 23807 usec)\n",
      "  Server: \n",
      "    Inference count: 504\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 23601 usec (overhead 68 usec + queue 58 usec + compute input 27 usec + compute infer 23439 usec + compute output 8 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 167.928 infer/sec, latency 23888 usec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 264\n",
      "    Throughput: 87.962 infer/sec\n",
      "    p50 latency: 11360 usec\n",
      "    p85 latency: 11405 usec\n",
      "    p90 latency: 11423 usec\n",
      "    p95 latency: 11434 usec\n",
      "    p99 latency: 11469 usec\n",
      "    Avg gRPC time: 11361 usec ((un)marshal request/response 2 usec + response wait 11359 usec)\n",
      "  Server: \n",
      "    Inference count: 264\n",
      "    Execution count: 264\n",
      "    Successful request count: 264\n",
      "    Avg request latency: 11233 usec (overhead 69 usec + queue 193 usec + compute input 24 usec + compute infer 10939 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 87.962 infer/sec, latency 11405 usec\n",
      "CPU times: user 1.93 s, sys: 898 ms, total: 2.83 s\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# warm up model with some inferences for faster analysis  (takes about 5 minutes)\n",
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "batchSize = 8\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}\n",
    "batchSize = 4\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}\n",
    "batchSize = 1\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-dynbatch\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 86.9664 infer/sec. p85 latency: 11479 usec\n",
      "  Pass [2] throughput: 87.9597 infer/sec. p85 latency: 11404 usec\n",
      "  Pass [3] throughput: 87.9628 infer/sec. p85 latency: 11401 usec\n",
      "  Client: \n",
      "    Request count: 263\n",
      "    Throughput: 87.6296 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 87.96 infer/sec\n",
      "    p50 latency: 11367 usec\n",
      "    p85 latency: 11421 usec\n",
      "    p90 latency: 11440 usec\n",
      "    p95 latency: 11493 usec\n",
      "    p99 latency: 11625 usec\n",
      "    Avg gRPC time: 11373 usec (marshal 2 usec + response wait 11370 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 263\n",
      "    Execution count: 263\n",
      "    Successful request count: 263\n",
      "    Avg request latency: 11229 usec (overhead 67 usec + queue 188 usec + compute input 24 usec + compute infer 10941 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 89.9669 infer/sec. p85 latency: 22208 usec\n",
      "  Pass [2] throughput: 90.9414 infer/sec. p85 latency: 22207 usec\n",
      "  Pass [3] throughput: 89.9618 infer/sec. p85 latency: 22170 usec\n",
      "  Client: \n",
      "    Request count: 271\n",
      "    Throughput: 90.2901 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 90.62 infer/sec\n",
      "    p50 latency: 22174 usec\n",
      "    p85 latency: 22199 usec\n",
      "    p90 latency: 22208 usec\n",
      "    p95 latency: 22235 usec\n",
      "    p99 latency: 22284 usec\n",
      "    Avg gRPC time: 22083 usec (marshal 3 usec + response wait 22079 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 271\n",
      "    Execution count: 271\n",
      "    Successful request count: 271\n",
      "    Avg request latency: 21918 usec (overhead 69 usec + queue 10859 usec + compute input 27 usec + compute infer 10955 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 112.956 infer/sec. p85 latency: 26574 usec\n",
      "  Pass [2] throughput: 111.952 infer/sec. p85 latency: 26602 usec\n",
      "  Pass [3] throughput: 111.943 infer/sec. p85 latency: 26644 usec\n",
      "  Client: \n",
      "    Request count: 337\n",
      "    Throughput: 112.283 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 112.62 infer/sec\n",
      "    p50 latency: 26541 usec\n",
      "    p85 latency: 26615 usec\n",
      "    p90 latency: 26636 usec\n",
      "    p95 latency: 26670 usec\n",
      "    p99 latency: 37486 usec\n",
      "    Avg gRPC time: 26623 usec (marshal 3 usec + response wait 26619 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 337\n",
      "    Execution count: 227\n",
      "    Successful request count: 337\n",
      "    Avg request latency: 26418 usec (overhead 90 usec + queue 12536 usec + compute input 36 usec + compute infer 13747 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 130.933 infer/sec. p85 latency: 30624 usec\n",
      "  Pass [2] throughput: 128.947 infer/sec. p85 latency: 30626 usec\n",
      "  Pass [3] throughput: 129.928 infer/sec. p85 latency: 30618 usec\n",
      "  Client: \n",
      "    Request count: 390\n",
      "    Throughput: 129.936 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 130.27 infer/sec\n",
      "    p50 latency: 30535 usec\n",
      "    p85 latency: 30620 usec\n",
      "    p90 latency: 30646 usec\n",
      "    p95 latency: 30776 usec\n",
      "    p99 latency: 41862 usec\n",
      "    Avg gRPC time: 30802 usec (marshal 2 usec + response wait 30799 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 390\n",
      "    Execution count: 199\n",
      "    Successful request count: 390\n",
      "    Avg request latency: 30555 usec (overhead 104 usec + queue 15399 usec + compute input 39 usec + compute infer 15003 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 137.949 infer/sec. p85 latency: 45648 usec\n",
      "  Pass [2] throughput: 134.936 infer/sec. p85 latency: 45837 usec\n",
      "  Pass [3] throughput: 138.935 infer/sec. p85 latency: 45669 usec\n",
      "  Client: \n",
      "    Request count: 412\n",
      "    Throughput: 137.273 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 137.61 infer/sec\n",
      "    p50 latency: 35223 usec\n",
      "    p85 latency: 45737 usec\n",
      "    p90 latency: 45834 usec\n",
      "    p95 latency: 46276 usec\n",
      "    p99 latency: 50449 usec\n",
      "    Avg gRPC time: 36340 usec (marshal 3 usec + response wait 36336 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 412\n",
      "    Execution count: 181\n",
      "    Successful request count: 412\n",
      "    Avg request latency: 36105 usec (overhead 124 usec + queue 18761 usec + compute input 41 usec + compute infer 17168 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 143.951 infer/sec. p85 latency: 50478 usec\n",
      "  Pass [2] throughput: 145.936 infer/sec. p85 latency: 44245 usec\n",
      "  Pass [3] throughput: 150.907 infer/sec. p85 latency: 39559 usec\n",
      "  Client: \n",
      "    Request count: 441\n",
      "    Throughput: 146.932 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 147.26 infer/sec\n",
      "    p50 latency: 39470 usec\n",
      "    p85 latency: 44218 usec\n",
      "    p90 latency: 50411 usec\n",
      "    p95 latency: 50626 usec\n",
      "    p99 latency: 59543 usec\n",
      "    Avg gRPC time: 40583 usec (marshal 3 usec + response wait 40579 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 441\n",
      "    Execution count: 156\n",
      "    Successful request count: 441\n",
      "    Avg request latency: 40351 usec (overhead 148 usec + queue 20004 usec + compute input 41 usec + compute infer 20145 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 146.945 infer/sec. p85 latency: 55004 usec\n",
      "  Pass [2] throughput: 148.915 infer/sec. p85 latency: 54552 usec\n",
      "  Pass [3] throughput: 150.919 infer/sec. p85 latency: 54924 usec\n",
      "  Client: \n",
      "    Request count: 447\n",
      "    Throughput: 148.926 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 149.26 infer/sec\n",
      "    p50 latency: 48321 usec\n",
      "    p85 latency: 54917 usec\n",
      "    p90 latency: 59178 usec\n",
      "    p95 latency: 67939 usec\n",
      "    p99 latency: 72209 usec\n",
      "    Avg gRPC time: 47244 usec (marshal 3 usec + response wait 47240 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 447\n",
      "    Execution count: 139\n",
      "    Successful request count: 447\n",
      "    Avg request latency: 47017 usec (overhead 170 usec + queue 22807 usec + compute input 44 usec + compute infer 23982 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 150.946 infer/sec. p85 latency: 63674 usec\n",
      "  Pass [2] throughput: 151.918 infer/sec. p85 latency: 53053 usec\n",
      "  Pass [3] throughput: 154.927 infer/sec. p85 latency: 53005 usec\n",
      "  Client: \n",
      "    Request count: 458\n",
      "    Throughput: 152.597 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 152.93 infer/sec\n",
      "    p50 latency: 52809 usec\n",
      "    p85 latency: 53087 usec\n",
      "    p90 latency: 63248 usec\n",
      "    p95 latency: 72502 usec\n",
      "    p99 latency: 76956 usec\n",
      "    Avg gRPC time: 51673 usec (marshal 3 usec + response wait 51669 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 458\n",
      "    Execution count: 123\n",
      "    Successful request count: 458\n",
      "    Avg request latency: 51439 usec (overhead 185 usec + queue 24613 usec + compute input 46 usec + compute infer 26580 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 156.931 infer/sec. p85 latency: 57640 usec\n",
      "  Pass [2] throughput: 160.917 infer/sec. p85 latency: 57538 usec\n",
      "  Pass [3] throughput: 155.925 infer/sec. p85 latency: 57432 usec\n",
      "  Client: \n",
      "    Request count: 474\n",
      "    Throughput: 157.924 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 158.26 infer/sec\n",
      "    p50 latency: 57255 usec\n",
      "    p85 latency: 57523 usec\n",
      "    p90 latency: 57654 usec\n",
      "    p95 latency: 76878 usec\n",
      "    p99 latency: 88172 usec\n",
      "    Avg gRPC time: 57304 usec (marshal 3 usec + response wait 57300 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 474\n",
      "    Execution count: 110\n",
      "    Successful request count: 474\n",
      "    Avg request latency: 57025 usec (overhead 176 usec + queue 26303 usec + compute input 49 usec + compute infer 30482 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 157.945 infer/sec. p85 latency: 61871 usec\n",
      "  Pass [2] throughput: 167.906 infer/sec. p85 latency: 65776 usec\n",
      "  Pass [3] throughput: 158.926 infer/sec. p85 latency: 65953 usec\n",
      "  Client: \n",
      "    Request count: 485\n",
      "    Throughput: 161.593 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 161.93 infer/sec\n",
      "    p50 latency: 61133 usec\n",
      "    p85 latency: 65747 usec\n",
      "    p90 latency: 76939 usec\n",
      "    p95 latency: 85449 usec\n",
      "    p99 latency: 96671 usec\n",
      "    Avg gRPC time: 61947 usec (marshal 3 usec + response wait 61943 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 485\n",
      "    Execution count: 102\n",
      "    Successful request count: 485\n",
      "    Avg request latency: 61646 usec (overhead 183 usec + queue 27625 usec + compute input 52 usec + compute infer 33768 usec + compute output 17 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 87.6296 infer/sec, latency 11421 usec\n",
      "Concurrency: 2, throughput: 90.2901 infer/sec, latency 22199 usec\n",
      "Concurrency: 3, throughput: 112.283 infer/sec, latency 26615 usec\n",
      "Concurrency: 4, throughput: 129.936 infer/sec, latency 30620 usec\n",
      "Concurrency: 5, throughput: 137.273 infer/sec, latency 45737 usec\n",
      "Concurrency: 6, throughput: 146.932 infer/sec, latency 44218 usec\n",
      "Concurrency: 7, throughput: 148.926 infer/sec, latency 54917 usec\n",
      "Concurrency: 8, throughput: 152.597 infer/sec, latency 53087 usec\n",
      "Concurrency: 9, throughput: 157.924 infer/sec, latency 57523 usec\n",
      "Concurrency: 10, throughput: 161.593 infer/sec, latency 65747 usec\n",
      "CPU times: user 321 ms, sys: 128 ms, total: 450 ms\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBa77GVBDoQZ"
   },
   "source": [
    "지연 시간과 처리량 모두에서 상당한 개선이 보여야 합니다. \n",
    "* 바닐라 ONNX 구성 또는 바닐라 TorchScript와 비교할 때 그 영향은 얼마나 큽니까? \n",
    "* 여러 인스턴스를 구현할 때 병목 현상이 생긴 이유는 무엇이라고 생각하십니까?\n",
    "\n",
    "강사와 결과를 논의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAxgDNrtDoQZ"
   },
   "source": [
    "<h3 style=\"color:green;\">축하합니다!</h3><br>\n",
    "다음을 사용하여 GPU 활용률을 개선하고 지연 시간을 줄이는 몇 가지 전략을 배웠습니다.\n",
    "\n",
    "* 동시 모델 실행\n",
    "* 스케줄링\n",
    "* 동적 배치\n",
    "\n",
    "이 다음 수업에서는 여러 동시 실행 수준에서 추론 성능을 보다 공식적으로 평가하고 체계적인 방식으로 추론 성능을 분석하는 방법에 대해 알아보겠습니다. 다음 노트북을 진행하십시오.<br>\n",
    "[3.0 서버 성능](030_ServerPerformance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4zDZXHKDoQZ"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"헤더\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "020_HostingTheModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
